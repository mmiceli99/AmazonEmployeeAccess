# load packages here
library(tidyverse)
library(ggfortify)  # plot lm objects using ggplot instead of base R
library(car)  # needed for added-variable plots and dfbetas
library(corrplot)  # colored correlation matrix
# install.packages("devtools")
# devtools::install_github("thomasp85/patchwork")
library(patchwork)
bptest(bodyfat_lm)
# load packages here
library(tidyverse)
library(ggfortify)  # plot lm objects using ggplot instead of base R
library(car)  # needed for added-variable plots and dfbetas
library(corrplot)  # colored correlation matrix
# install.packages("devtools")
# devtools::install_github("thomasp85/patchwork")
library(patchwork)
bptest(bodyfat_lm)
library(lmtest)
# load packages here
library(tidyverse)
library(ggfortify)  # plot lm objects using ggplot instead of base R
install.packages('ggfortify')
install.packages('car')
install.packages('corrplot')
install.packages('patchwork')
# load packages here
library(tidyverse)
library(ggfortify)  # plot lm objects using ggplot instead of base R
library(car)  # needed for added-variable plots and dfbetas
library(corrplot)  # colored correlation matrix
# install.packages("devtools")
# devtools::install_github("thomasp85/patchwork")
library(patchwork)
super <- read_table("C:/School/Stat 330/Supervisor.txt")
summary(super)
# load packages here
library(tidyverse)
library(ggfortify)  # plot lm objects using ggplot instead of base R
library(car)  # needed for added-variable plots and dfbetas
library(corrplot)  # colored correlation matrix
# install.packages("devtools")
# devtools::install_github("thomasp85/patchwork")
library(patchwork)
super <- read_table("C:/School/Stat 330/Supervisor.txt")
summary(super)
# plot(super, pch = 19)  # both do the same thing
pairs(super, pch = 19)
round(cor(super), 2)
corrplot(cor(super), type = "upper")
# super_lm <- lm(Rating ~ Complaints + Privileges + Learn +
#                  Raises + Critical + Advance, data = super)
super_lm <- lm(Rating ~ ., data = super) #@ the . gets all cols in df but the one before ~
summary(super_lm)
super$residuals <- super_lm$residuals
library(tidyverse)
library(ggfortify)  # plot lm objects using ggplot instead of base R
library(car)  # needed for added-variable plots and dfbetas
library(corrplot)  # colored correlation matrix
# install.packages("devtools")
# devtools::install_github("thomasp85/patchwork")
library(patchwork)
data <- vroom('C:/Users/ileci/Downloads/perc_io.csv')
library(vroom)
data <- vroom('C:/Users/ileci/Downloads/perc_io.csv')
View(data)
newDf <- na.omit(newDF)
newDf <- na.omit(data)
View(newDf)
inv  <- na.omit(data)
summary(inv)
pairs(inv, pch = 19)
plot(inv, pch = 19)
inv_lm <- lm(shifts_per_month ~ perc.io + industry + po_shifts + io_shifts,
data = super) #@ the . gets all cols in df but the one before ~
inv_lm <- lm(inv$shifts_per_month ~ perc.io + industry + po_shifts + io_shifts,
data = super) #@ the . gets all cols in df but the one before ~
inv_lm <- lm(shifts_per_month ~ inv$ + industry + po_shifts + io_shifts,
inv_lm <- lm(shifts_per_month ~ perc_io + industry + po_shifts + io_shifts,
data = inv) #@ the . gets all cols in df but the one before ~
summary(inv_lm)
inv_vars <- inv[shifts_per_month, perc_io, industry, po_shifts, io_shifts]
inv_vars <- inv['shifts_per_month', perc_io, industry, po_shifts, io_shifts]
inv_vars <- inv['shifts_per_month', 'perc_io', 'industry', 'po_shifts', 'io_shifts']
inv_vars <- inv %>%
select(shifts_per_month, perc_io, industry, po_shifts, io_shifts)
inv_lm <- lm(shifts_per_month ~ *,
inv_lm <- lm(shifts_per_month ~ .,
data = inv) #@ the . gets all cols in df but the one before ~
summary(inv_lm)
inv_lm <- lm(shifts_per_month ~ .,
data = inv_vars) #@ the . gets all cols in df but the one before ~
summary(inv_lm)
inv_vars <- inv %>%
select(shifts_per_month, perc_io, industry, po_shifts, io_shifts) %>%
mutate(industry = as.factor(industry))
inv_lm <- lm(shifts_per_month ~ .,
data = inv_vars) #@ the . gets all cols in df but the one before ~
summary(inv_lm)
plot(x=inv.perc_io, y=shifts_per_month)
plot(x=inv$perc_io, y=inf$shifts_per_month)
plot(x=inv$perc_io, y=inv$shifts_per_month)
# load packages here
library(tidyverse)
# Note: code all from Module 1
cars = read_table("MPGData.txt")
ggplot(inv) +
geom_point(x = perc_io, y = shifts_per_month)
ggplot(data = inv) +
geom_point(x = perc_io, y = shifts_per_month)
ggplot(data = inv) +
geom_point(x = inv$perc_io, y = shifts_per_month)
ggplot(data = inv) +
geom_point(x = inv$perc_io, y = inv$shifts_per_month)
ggplot(data = inv) +
geom_point(mapping = (x = inv$perc_io, y = inv$shifts_per_month))
ggplot(data = inv) +
geom_point(mapping = aes(x = inv$perc_io, y = inv$shifts_per_month))
ggplot(data = inv) +
geom_point(mapping = aes(x = perc_io, y = shifts_per_month))
ggplot(data = inv) +
geom_point(mapping = aes(x = perc_io, y = shifts_per_month), col = industry)
ggplot(data = inv) +
geom_point(mapping = aes(x = perc_io, y = shifts_per_month), col = inv$industry)
ggplot(data = inv) +
geom_point(mapping = aes(x = perc_io, y = shifts_per_month))
ggplot(data = inv) +
geom_point(mapping = aes(x = perc_io, y = shifts_per_month, col = industry))
ggplot(data = inv) +
geom_point(mapping = aes(x = perc_io, y = shifts_per_month))
ggplot(data = inv) +
geom_point(mapping = aes(x = perc_io, y = shifts_per_month)) +
geom_line(mapping = aes(x = perc_io,
y = predict(inv_lm)#,
color = Education,
ggplot(data = inv) +
geom_line(mapping = aes(x = perc_io,
ggplot(data = inv) +
geom_point(mapping = aes(x = perc_io, y = shifts_per_month)) +
geom_line(mapping = aes(x = perc_io,
y = predict(inv_lm)))#,
ggplot(data = inv) +
ggplot(data = inv) +
geom_point(mapping = aes(x = perc_io, y = shifts_per_month)) +
geom_line(mapping = aes(x = perc_io,
y = predict(inv_lm)))#,
ggplot(data = inv) +
geom_point(mapping = aes(x = perc_io, y = shifts_per_month))
inv_vars <- inv %>%
select(shifts_per_month, perc_io, industry, po_shifts, io_shifts, avg_headcount) %>%
mutate(industry = as.factor(industry))
inv_lm <- lm(shifts_per_month ~ .,
data = inv_vars) #@ the . gets all cols in df but the one before ~
summary(inv_lm)
library(vroom)
library(tidyverse)
library(tidymodels)
library(embed)
library(discrim)
library(naivebayes)
library(kknn)
library(doParallel)
#parallel::detectCores() #How many cores do I have?
cl <- makePSOCKcluster(10) # num_cores to use
registerDoParallel(cl)
ama_train <- vroom("./train.csv")
setwd("C:/School/Stat348/AmazonEmployeeAccess")
library(vroom)
library(tidyverse)
library(tidymodels)
library(embed)
library(discrim)
library(naivebayes)
library(kknn)
library(doParallel)
#parallel::detectCores() #How many cores do I have?
cl <- makePSOCKcluster(10) # num_cores to use
registerDoParallel(cl)
ama_train <- vroom("./train.csv")
ama_test <- vroom("./test.csv")
ama_train <- ama_train %>%
mutate(ACTION = as.factor(ACTION))
my_recipe <- recipe(ACTION ~ ., data=ama_train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
#step_mutate_at(ACTION, fn = factor) %>%
step_other(all_nominal_predictors(), threshold = .001) %>%
step_normalize(all_predictors()) %>%# combines categorical values that occur <5% into an "other" value
#step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>% #target encoding
step_normalize(all_predictors()) %>%
#step_pca(all_predictors(), threshold=.9) #Threshold is between 0 and 1#target encoding
# also step_lencode_glm() and step_lencode_bayes()
# NOTE: some of these step functions are not appropriate to use together
# apply the recipe to your data
prep <- prep(my_recipe)
library(vroom)
library(tidyverse)
library(tidymodels)
library(embed)
library(discrim)
library(naivebayes)
library(kknn)
library(doParallel)
#parallel::detectCores() #How many cores do I have?
cl <- makePSOCKcluster(10) # num_cores to use
registerDoParallel(cl)
ama_train <- vroom("./train.csv")
ama_test <- vroom("./test.csv")
ama_train <- ama_train %>%
mutate(ACTION = as.factor(ACTION))
my_recipe <- recipe(ACTION ~ ., data=ama_train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
#step_mutate_at(ACTION, fn = factor) %>%
step_other(all_nominal_predictors(), threshold = .001) %>%
step_normalize(all_predictors()) %>%# combines categorical values that occur <5% into an "other" value
#step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>% #target encoding
step_normalize(all_predictors()) #%>%
#step_pca(all_predictors(), threshold=.9) #Threshold is between 0 and 1#target encoding
# also step_lencode_glm() and step_lencode_bayes()
# NOTE: some of these step functions are not appropriate to use together
# apply the recipe to your data
prep <- prep(my_recipe)
ama_train <- ama_train %>%
mutate(ACTION = as.factor(ACTION))
my_recipe <- recipe(ACTION ~ ., data=ama_train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
#step_mutate_at(ACTION, fn = factor) %>%
step_other(all_nominal_predictors(), threshold = .001) %>%
step_normalize(all_predictors()) %>%# combines categorical values that occur <5% into an "other" value
#step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>% #target encoding
step_normalize(all_predictors()) #%>%
# apply the recipe to your data
prep <- prep(my_recipe)
my_recipe <- recipe(ACTION ~ ., data=ama_train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
#step_mutate_at(ACTION, fn = factor) %>%
step_other(all_nominal_predictors(), threshold = .001) %>%
step_normalize(all_predictors()) %>%# combines categorical values that occur <5% into an "other" value
#step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>% #target encoding
#step_normalize(all_predictors()) #%>%
#step_pca(all_predictors(), threshold=.9) #Threshold is between 0 and 1#target encoding
# also step_lencode_glm() and step_lencode_bayes()
# NOTE: some of these step functions are not appropriate to use together
# apply the recipe to your data
prep <- prep(my_recipe)
baked <- bake(prep, new_data = ama_train)
ama_train <- ama_train %>%
mutate(ACTION = as.factor(ACTION))
my_recipe <- recipe(ACTION ~ ., data=ama_train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
#step_mutate_at(ACTION, fn = factor) %>%
step_other(all_nominal_predictors(), threshold = .001) %>%
step_normalize(all_predictors()) %>%# combines categorical values that occur <5% into an "other" value
#step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>% #target encoding
#step_normalize(all_predictors()) #%>%
#step_pca(all_predictors(), threshold=.9) #Threshold is between 0 and 1#target encoding
# also step_lencode_glm() and step_lencode_bayes()
# NOTE: some of these step functions are not appropriate to use together
# apply the recipe to your data
prep <- prep(my_recipe)
baked <- bake(prep, new_data = ama_train)
my_recipe <- recipe(ACTION ~ ., data=ama_train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
#step_mutate_at(ACTION, fn = factor) %>%
step_other(all_nominal_predictors(), threshold = .001) %>%
step_normalize(all_predictors()) %>%# combines categorical values that occur <5% into an "other" value
#step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) #%>% #target encoding
# apply the recipe to your data
prep <- prep(my_recipe)
baked <- bake(prep, new_data = ama_train)
stopCluster(cl)
my_recipe <- recipe(ACTION ~ ., data=ama_train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
#step_mutate_at(ACTION, fn = factor) %>%
step_other(all_nominal_predictors(), threshold = .001) %>%
step_normalize(all_predictors()) %>%# combines categorical values that occur <5% into an "other" value
#step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>% #target encoding
step_normalize(all_predictors()) %>%
step_pca(all_predictors(), threshold=.9) #Threshold is between 0 and 1#target encoding
# apply the recipe to your data
prep <- prep(my_recipe)
baked <- bake(prep, new_data = ama_train)
ama_train <- ama_train %>%
mutate(ACTION = as.factor(ACTION))
my_recipe <- recipe(ACTION ~ ., data=ama_train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
#step_mutate_at(ACTION, fn = factor) %>%
step_other(all_nominal_predictors(), threshold = .001) %>%
#step_normalize(all_predictors()) %>%# combines categorical values that occur <5% into an "other" value
#step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>% #target encoding
step_normalize(all_predictors()) %>%
step_pca(all_predictors(), threshold=.9) #Threshold is between 0 and 1#target encoding
# apply the recipe to your data
prep <- prep(my_recipe)
baked <- bake(prep, new_data = ama_train)
library(vroom)
library(tidyverse)
library(tidymodels)
library(embed)
library(discrim)
library(naivebayes)
library(kknn)
library(doParallel)
#parallel::detectCores() #How many cores do I have?
cl <- makePSOCKcluster(10) # num_cores to use
registerDoParallel(cl)
ama_train <- vroom("./train.csv")
ama_test <- vroom("./test.csv")
ama_train <- ama_train %>%
mutate(ACTION = as.factor(ACTION))
my_recipe <- recipe(ACTION ~ ., data=ama_train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
#step_mutate_at(ACTION, fn = factor) %>%
step_other(all_nominal_predictors(), threshold = .001) %>%
#step_normalize(all_predictors()) %>%# combines categorical values that occur <5% into an "other" value
#step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>% #target encoding
step_normalize(all_predictors()) %>%
step_pca(all_predictors(), threshold=.9) #Threshold is between 0 and 1#target encoding
# also step_lencode_glm() and step_lencode_bayes()
# NOTE: some of these step functions are not appropriate to use together
# apply the recipe to your data
prep <- prep(my_recipe)
baked <- bake(prep, new_data = ama_train)
## poly model
svmPoly <- svm_poly(degree=tune(), cost=tune()) %>% # set or tune
set_mode("classification") %>%
set_engine("kernlab")
Poly_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(svmPoly_model)
library(vroom)
library(tidyverse)
library(tidymodels)
library(embed)
library(discrim)
library(naivebayes)
library(kknn)
library(doParallel)
#parallel::detectCores() #How many cores do I have?
cl <- makePSOCKcluster(10) # num_cores to use
registerDoParallel(cl)
ama_train <- vroom("./train.csv")
ama_test <- vroom("./test.csv")
ama_train <- ama_train %>%
mutate(ACTION = as.factor(ACTION))
my_recipe <- recipe(ACTION ~ ., data=ama_train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
#step_mutate_at(ACTION, fn = factor) %>%
step_other(all_nominal_predictors(), threshold = .001) %>%
#step_normalize(all_predictors()) %>%# combines categorical values that occur <5% into an "other" value
#step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>% #target encoding
step_normalize(all_predictors()) %>%
step_pca(all_predictors(), threshold=.9) #Threshold is between 0 and 1#target encoding
# also step_lencode_glm() and step_lencode_bayes()
# NOTE: some of these step functions are not appropriate to use together
# apply the recipe to your data
prep <- prep(my_recipe)
baked <- bake(prep, new_data = ama_train)
## poly model
svmPoly <- svm_poly(degree=tune(), cost=tune()) %>% # set or tune
set_mode("classification") %>%
set_engine("kernlab")
Poly_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(svmPoly)
## Grid of values to tune over
tuning_grid <- grid_regular(degree(), cost(),
levels = 5) ## L^2 total tuning possibilities
## Split data for CV
folds <- vfold_cv(ama_train, v = 5, repeats=1)
## Run the CV
CV_results <- Poly_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(roc_auc)) #Or leave metrics NULL
install.packages('kernlab')
library(vroom)
library(tidyverse)
library(tidymodels)
library(embed)
library(discrim)
library(naivebayes)
library(kknn)
library(doParallel)
#parallel::detectCores() #How many cores do I have?
cl <- makePSOCKcluster(10) # num_cores to use
registerDoParallel(cl)
ama_train <- vroom("./train.csv")
ama_test <- vroom("./test.csv")
ama_train <- ama_train %>%
mutate(ACTION = as.factor(ACTION))
my_recipe <- recipe(ACTION ~ ., data=ama_train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
#step_mutate_at(ACTION, fn = factor) %>%
step_other(all_nominal_predictors(), threshold = .001) %>%
#step_normalize(all_predictors()) %>%# combines categorical values that occur <5% into an "other" value
#step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>% #target encoding
step_normalize(all_predictors()) %>%
step_pca(all_predictors(), threshold=.9) #Threshold is between 0 and 1#target encoding
# also step_lencode_glm() and step_lencode_bayes()
# NOTE: some of these step functions are not appropriate to use together
# apply the recipe to your data
prep <- prep(my_recipe)
baked <- bake(prep, new_data = ama_train)
## poly model
svmPoly <- svm_poly(degree=tune(), cost=tune()) %>% # set or tune
set_mode("classification") %>%
set_engine("kernlab")
Poly_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(svmPoly)
## Grid of values to tune over
tuning_grid <- grid_regular(degree(), cost(),
levels = 5) ## L^2 total tuning possibilities
## Split data for CV
folds <- vfold_cv(ama_train, v = 5, repeats=1)
## Run the CV
CV_results <- Poly_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(roc_auc)) #Or leave metrics NULL
stopCluster(cl)
